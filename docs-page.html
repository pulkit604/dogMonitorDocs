<!DOCTYPE html>
<html lang="en">

<head>
	<title>Smart Dog Monitor - Documentation</title>

	<!-- Meta -->
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<meta name="description" content="Bootstrap 4 Template For Software Startups">
	<meta name="author" content="Xiaoying Riley at 3rd Wave Media">
	<link rel="shortcut icon" href="favicon.ico">

	<!-- Google Font -->
	<link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700&display=swap" rel="stylesheet">

	<!-- FontAwesome JS-->
	<script defer src="assets/fontawesome/js/all.min.js"></script>

	<!-- Plugins CSS -->
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.2/styles/atom-one-dark.min.css">

	<!-- Theme CSS -->
	<link id="theme-style" rel="stylesheet" href="assets/css/theme.css">

</head>

<body class="docs-page">
	<header class="header fixed-top">
		<div class="branding docs-branding">
			<div class="container-fluid position-relative py-2">
				<div class="docs-logo-wrapper">
					<button id="docs-sidebar-toggler" class="docs-sidebar-toggler docs-sidebar-visible mr-2 d-xl-none"
						type="button">
						<span></span>
						<span></span>
						<span></span>
					</button>
					<div class="site-logo"><a class="navbar-brand" href="index.html"><img class="logo-icon mr-2"
								src="assets/images/coderdocs-logo.svg" alt="logo"><span class="logo-text">Home<span
									class="text-alt"></span></span></a></div>
				</div>
				<!--//docs-logo-wrapper-->
				<div class="docs-top-utilities d-flex justify-content-end align-items-center">
					<div class="top-search-box d-none d-lg-flex">
						<!-- <form class="search-form">
				            <input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
				            <button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
				        </form> -->
					</div>

					<!-- <ul class="social-list list-inline mx-md-3 mx-lg-5 mb-0 d-none d-lg-flex">
						<li class="list-inline-item"><a href="#"><i class="fab fa-github fa-fw"></i></a></li>
			            <li class="list-inline-item"><a href="#"><i class="fab fa-twitter fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="#"><i class="fab fa-slack fa-fw"></i></a></li>
		                <li class="list-inline-item"><a href="#"><i class="fab fa-product-hunt fa-fw"></i></a></li>
		            </ul>//social-list -->
					<!-- <a href="https://themes.3rdwavemedia.com/bootstrap-templates/startup/coderdocs-free-bootstrap-4-documentation-template-for-software-projects/" class="btn btn-primary d-none d-lg-flex">Download</a> -->
				</div>
				<!--//docs-top-utilities-->
			</div>
			<!--//container-->
		</div>
		<!--//branding-->
	</header>
	<!--//header-->

	<div class="docs-wrapper">
		<div id="docs-sidebar" class="docs-sidebar">
			<div class="top-search-box d-lg-none p-3">
				<form class="search-form">
					<input type="text" placeholder="Search the docs..." name="search" class="form-control search-input">
					<button type="submit" class="btn search-btn" value="Search"><i class="fas fa-search"></i></button>
				</form>
			</div>
			<nav id="docs-nav" class="docs-nav navbar">
				<ul class="section-items list-unstyled nav flex-column pb-3">
					<li class="nav-item section-title"><a class="nav-link scrollto active" href="#section-1"><span
								class="theme-icon-holder mr-2"><i class="fas fa-map-signs"></i></span>Introduction</a>
					</li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-1-1">Idea</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-1-2">Software Architecture and Workflow</a></li>
					<li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-2"><span
								class="theme-icon-holder mr-2"><i class="fas fa-tools"></i></span>Tools and
							Resources</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-2-1">Raspberry Pi 4 B</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-2-2">Anker Soundcore 2</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-2-3">Gyvazla USB-microphone</a></li>
					<li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-3"><span
								class="theme-icon-holder mr-2"><i class="fas fa-cogs fa-fw"></i></span>Technology Stack</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-3-1">Python</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-3-2">Tensorflow</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-3-3">Firebase Cloud Messaging</a></li>
					<li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-4"><span
								class="theme-icon-holder mr-2"><i class="fas fa-desktop"></i></span>Demo 1</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-4-1">Setting up Raspberry</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-4-2">Microphone Setup</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-4-3">Audio Output</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-4-4">Data collection for Tensorflow</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-4-5">Test Api Structure</a></li>
					<li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-5"><span
								class="theme-icon-holder mr-2"><i class="fas fa-desktop"></i></span>Demo 2</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-5-1">Problems in Demo 1</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-5-2">Mic + Speaker Function</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-5-3">Full Use Case</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-5-4">Python script to showcase features</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-5-5">Wesbite for Push Notifications</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-5-6">Data collection update</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-5-7">Bitrate change</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-5-8">Tensorflow model update</a></li>
					<li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-6"><span
								class="theme-icon-holder mr-2"><i class="fas fa-tablet-alt"></i></span>Final Version</a>
					</li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-6-1">Dataset Issues and final workaround</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-6-2">Final Model build and results</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-6-3">Model Deploy and Flask Server</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-6-4">Continous Listening and Break Script</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-6-5">Final Demo</a></li>
					<li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-7"><span
							class="theme-icon-holder mr-2"><i class="fas fa-file-code"></i></span>Python Scripts</a>
					</li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-7-1">Audioset Downloader</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-7-2">Bitrate Converter</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-7-3">Crop From Timestamp</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-7-4">Set Mono Sound Channel</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-7-5">Strip to 9 Seconds</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-7-6">Flask server</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-7-7">Final script for Raspberry Pi</a></li>
					<li class="nav-item"><a class="nav-link scrollto" href="#item-7-8">Audio classifier Model</a></li>
					<li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-8"><span
							class="theme-icon-holder mr-2"><i class="fas fa-microscope"></i></span>Observations and Future Scope</a>
					</li>
					<li class="nav-item section-title mt-3"><a class="nav-link scrollto" href="#section-9"><span
							class="theme-icon-holder mr-2"><i class="fas fa-bookmark"></i></span>References</a>
					</li>
				</ul>

			</nav>
			<!--//docs-nav-->
		</div>
		<!--//docs-sidebar-->
		<div class="docs-content">
			<div class="container">
				<article class="docs-article" id="section-1">
					<header class="docs-header">
						<h1 class="docs-heading">Introduction</h1>
						<section class="docs-intro">
							<p>This project was developed within the scope of the course "Introduction to Internet of
								Things" by Dr. Ing. Mohammed Al-Olofi. This documentation describes all processes carried out during the project development time as
								well as the tools and technologies and references used in the process.</p>
						</section>
						<!--//docs-intro-->

						<!-- <h5>Github Code Example:</h5> -->
						<!-- <p>You can <a class="theme-link" href="https://gist.github.com/"  target="_blank">embed your code snippets using Github gists</a></p> -->
						<!-- <div class="docs-code-block"> -->
						<!-- ** Embed github code starts ** -->
						<!-- <script src="https://gist.github.com/xriley/fce6cf71edfd2dadc7919eb9c98f3f17.js"></script> -->
						<!-- ** Embed github code ends ** -->
						<!-- </div>//docs-code-block -->

						<!-- <h5>Highlight.js Example:</h5> -->
						<!-- <p>You can <a class="theme-link" href="https://github.com/highlightjs/highlight.js" target="_blank">embed your code snippets using highlight.js</a> It supports <a class="theme-link" href="https://highlightjs.org/static/demo/" target="_blank">185 languages and 89 styles</a>.</p> -->
						<!-- <p>This template uses <a class="theme-link" href="https://highlightjs.org/static/demo/" target="_blank">Atom One Dark</a> style for the code blocks: <br><code>&#x3C;link rel=&#x22;stylesheet&#x22; href=&#x22;//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.2/styles/atom-one-dark.min.css&#x22;&#x3E;</code></p> -->
						<!-- <div class="docs-code-block">
							<pre class="shadow-lg rounded"><code class="json hljs">[
  {
    <span class="hljs-attr">"title"</span>: <span class="hljs-string">"apples"</span>,
    <span class="hljs-attr">"count"</span>: [<span class="hljs-number">12000</span>, <span class="hljs-number">20000</span>],
    <span class="hljs-attr">"description"</span>: {<span class="hljs-attr">"text"</span>: <span class="hljs-string">"..."</span>, <span class="hljs-attr">"sensitive"</span>: <span class="hljs-literal">false</span>}
  },
  {
    <span class="hljs-attr">"title"</span>: <span class="hljs-string">"oranges"</span>,
    <span class="hljs-attr">"count"</span>: [<span class="hljs-number">17500</span>, <span class="hljs-literal">null</span>],
    <span class="hljs-attr">"description"</span>: {<span class="hljs-attr">"text"</span>: <span class="hljs-string">"..."</span>, <span class="hljs-attr">"sensitive"</span>: <span class="hljs-literal">false</span>}
  }
]


</code></pre>
						</div>//docs-code-block -->


					</header>
					<section class="docs-section" id="item-1-1">
						<h2 class="section-heading">Idea</h2>
						<p>This project is about the implementation of a Smart Dog Monitor. There are situations in
							which a dog owner has to leave his dog alone and it suffers from separation anxiety or other
							conspicuous behavior. To tackle this problem this project was developed to monitor the behavior of
							the dog while he is alone at home. The Smart Dog Monitor is designed to detect the dog's
							emotional state based on the sounds it makes (barking, growling, yelping, etc.) and play
							calming music accordingly. If this calming music does not work, the dog owner will be
							notified by a push notification on his smartphone. The detection of the emotional state is
							done by a machine learning model that works on the concept of sound classification. The Raspberry Pi was used
							as the main device here along with a usb microphone and a bluetooth speaker. </p>


					</section>
					<!--//section-->

					<section class="docs-section" id="item-1-2">
						<h2 class="section-heading">Software Architecture and Workflow</h2>
						<p>Here is an overview of the Software architecture and Worflow of the project:</p>
						<figure class="figure docs-figure py-3">
							<a href="assets/images/Workflow.png"
								data-title="Software Architecture and Workflow" data-toggle="lightbox"><img
									class="figure-img img-fluid shadow rounded"
									src="assets/images/Workflow.png" alt=""
									style="width: 400px;"></a>
						</figure>
						<p>1. Here the noises of the dog are constantly listened to and stored via a microphone, which is connected to the Raspberry Pi. The sound recording is listened to and processed in the Raspberry Pi using Tensorflow, which is served on a Flask server.<br> </p>
						<p>2. When an anxious behavior of the dog is classified, an API call is made and calming music for the dog is played through the Bluetooth speakers. </p>
						<p>Steps 1 and 2 are repeated again if the dog is still making the sounds. </p>
						<p>3. If the dog does not calm down, a push notification is sent to the dog owner's smartphone via Firebase Cloud Messaging, informing them that the dog is showing unusual behavior.</p>
					</section>
					<!--//section-->
						<!-- <h5>Badges Examples:</h5>
						<div class="my-4">
							<span class="badge badge-primary">Primary</span>
							<span class="badge badge-secondary">Secondary</span>
							<span class="badge badge-success">Success</span>
							<span class="badge badge-danger">Danger</span>
							<span class="badge badge-warning">Warning</span>
							<span class="badge badge-info">Info</span>
							<span class="badge badge-light">Light</span>
							<span class="badge badge-dark">Dark</span>
						</div> -->
						<!-- <h5>Button Examples:</h5>
						<div class="row my-3">
							<div class="col-md-6 col-12">
								<ul class="list list-unstyled pl-0">
									<li><a href="#" class="btn btn-primary">Primary Button</a></li>
									<li><a href="#" class="btn btn-secondary">Secondary Button</a></li>
									<li><a href="#" class="btn btn-light">Light Button</a></li>
									<li><a href="#" class="btn btn-success">Succcess Button</a></li>
									<li><a href="#" class="btn btn-info">Info Button</a></li>
									<li><a href="#" class="btn btn-warning">Warning Button</a></li>
									<li><a href="#" class="btn btn-danger">Danger Button</a></li>
								</ul>
							</div> -->
							<!-- <div class="col-md-6 col-12">
								<ul class="list list-unstyled pl-0">
									<li><a href="#" class="btn btn-primary"><i class="fas fa-download mr-2"></i>
											Download Now</a></li>
									<li><a href="#" class="btn btn-secondary"><i class="fas fa-book mr-2"></i> View
											Docs</a></li>
									<li><a href="#" class="btn btn-light"><i
												class="fas fa-arrow-alt-circle-right mr-2"></i> View Features</a></li>
									<li><a href="#" class="btn btn-success"><i class="fas fa-code-branch mr-2"></i> Fork
											Now</a></li>
									<li><a href="#" class="btn btn-info"><i class="fas fa-play-circle mr-2"></i> Find
											Out Now</a></li>
									<li><a href="#" class="btn btn-warning"><i class="fas fa-bug mr-2"></i> Report
											Bugs</a></li>
									<li><a href="#" class="btn btn-danger"><i
												class="fas fa-exclamation-circle mr-2"></i> Submit Issues</a></li>
								</ul>
							</div>
						</div> -->
						<!--//row-->

						<!-- <h5>Progress Examples:</h5>
						<div class="my-4">
							<div class="progress my-4">
								<div class="progress-bar bg-success" role="progressbar" style="width: 25%"
									aria-valuenow="25" aria-valuemin="0" aria-valuemax="100"></div>
							</div>
							<div class="progress my-4">
								<div class="progress-bar bg-info" role="progressbar" style="width: 50%"
									aria-valuenow="50" aria-valuemin="0" aria-valuemax="100"></div>
							</div>
							<div class="progress my-4">
								<div class="progress-bar bg-warning" role="progressbar" style="width: 75%"
									aria-valuenow="75" aria-valuemin="0" aria-valuemax="100"></div>
							</div>
							<div class="progress my-4">
								<div class="progress-bar bg-danger" role="progressbar" style="width: 100%"
									aria-valuenow="100" aria-valuemin="0" aria-valuemax="100"></div>
							</div>
						</div> -->

					<!--//section-->


					<!--//section-->
					<!--//section-->

				</article>

				<article class="docs-article" id="section-2">
					<header class="docs-header">
						<h1 class="docs-heading">Tools and Resources</h1>
						<section class="docs-intro">
							<p>The various tools that were used for the project are listed below:</p>
						</section>
						<!--//docs-intro-->
					</header>
					<section class="docs-section" id="item-2-1">
						<h2 class="section-heading">Raspberry Pi 4 B</h2>
						<p>The Raspberry Pi 4 was used as the main device here. One of the major reason to choose a Raspberry Pi device was that we needed to use Tensorflow in our project for deep learning which is not widely available for micro-controllers but has recently been made compatible with certain models with high performance capabilities.</p>
						<figure class="figure docs-figure py-3">
							<a href="assets/images/Raspberry_Pi_4_Model_B_-_Side.jpeg"
								data-title="Raspberry Pi 4 Model B" data-toggle="lightbox"><img
									class="figure-img img-fluid shadow rounded"
									src="assets/images/Raspberry_Pi_4_Model_B_-_Side.jpeg" alt=""
									style="width: 400px;"></a>
							<figcaption class="figure-caption mt-3"></i>Raspberry Pi 4 Model B</figcaption>
						</figure>
						<p>To be able to use it for our purpose, an OS had to be installed. Here, the decision fell on
							Debian, since this operating system is compatible with most packages, has a certain
							stability and requires little maintenance.<br>
						Specifications of the product are as follows:
							<ul>
						<li>1,5 GHz ARM Cortex-A72 Quad-Core-CPU</li>
						<li>2 GB, 4 GB oder 8 GB LPDDR4 SDRAM</li>
						<li>Gigabit LAN RJ45 (mit bis zu 1000 Mbit)</li>
						<li>Bluetooth 5.0</li>
						<li>2x USB 2.0 / 2x USB 3.0</li>
						<li>2x microHDMI (1x 4k @60fps oder 2x 4k @30fps)</li>
						<li>USB Type-C @ 5V/3A</li>
					</ul></p>
						<figure class="figure docs-figure py-1">
							<a target="_blank" href="assets/images/debian.png" data-title="Debian OS on the Raspberry Pi"
								data-toggle="lightbox"><img class="figure-img img-fluid shadow rounded"
									src="assets/images/debian.png" alt="" style="width: 400px;"></a>
							<figcaption class="figure-caption mt-3"></i>Debian OS on the Raspberry Pi</figcaption>
						</figure>
						<br><a href="https://www.reichelt.de/raspberry-pi-4-b-4x-1-5-ghz-4-gb-ram-wlan-bt-rasp-pi-4-b-4gb-p259920.html?PROVID=2788&gclid=CjwKCAjwieuGBhAsEiwA1Ly_ndLs584wlayJsFP7E4mjFMFl_Zt9R8Hok9QNIex4uIMitx-luINYgRoCoHkQAvD_BwE">Purchase Link</a>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-2-2">
						<h2 class="section-heading">Anker Soundcore 2</h2>
						<p>The Anker Soundcore 2 was used here to play the soothing music. A connection to the device was established via Bluetooth. In order for the speaker to work, it is necessary to perform a few configurations here. <br> <br>
							<ol>
						<li>
							PulseAudio<br>
							PulseAudio is a network-capable sound server program distributed via the freedesktop.org project. It runs mainly on Linux, various BSD distributions such as FreeBSD and OpenBSD, macOS, as well as Illumos distributions and the Solaris operating system. <a href="https://wiki.debian.org/PulseAudio">( PulseAudio Wiki )</a> <br><br>
						</li>
						<li>
							Bluetooth connection<br>
							To establish a connection to the device the following steps had to be performed:<br><br>

							bluetoothctl<br>
							power on<br>
							agent on<br>
							default-agent<br><br>

							Then we found out the MAC address and could connect the device.<br>
							pair xx:xx:xx:xx:xx:xx<br>
							trust xx:xx:xx:xx:xx:xx<br>
							connect xx:xx:xx:xx:xx<br><br>
						</li>
						<li>
							A2DP support<br>
							After the connection to the device was established, the A2DP Bluetooth profile had to be set up to realize the sound output.

							For this the Bluetooth sound card had to be listed: <br><br>
							pacmd list-cards <br><br>

							The Bluetooth sound card was on index #1 and now we had to enable A2DP:<br><br>
							pacmd set-card-profile bluez_card.xx_xx_xx_xx_xx a2dp_sink<br><br>

							After that we had to set the speaker as output audio:<br><br>
							pacmd set-default-sink bluez_sink.xx_xx_xx_xx_xx.a2dp_sink
						</li>
					</ol>
							<figure class="figure docs-figure py-1">
								<a href="assets/images/anker.jpg" data-title="Anker Soundcore 2"
									data-toggle="lightbox"><img class="figure-img img-fluid shadow rounded"
										src="assets/images/anker.jpg" alt="" style="width: 400px;"></a>
								<figcaption class="figure-caption mt-3"></i>Anker Soundcore 2</figcaption>
							</figure>
						<br><a href="https://www.ebay.de/itm/384160458928?chn=ps&norover=1&mkevt=1&mkrid=707-134425-41852-0&mkcid=2&itemid=384160458928&targetid=1270839892511&device=c&mktype=pla&googleloc=9062657&poi=&campaignid=10203814797&mkgroupid=119187068301&rlsatarget=pla-1270839892511&abcId=1145991&merchantid=7364532&gclid=CjwKCAjwieuGBhAsEiwA1Ly_nZ8H26tSYDgPLlv8IY3G5h1zG3vI0s5nZawhchceeaa55tWOHVrBORoCZcEQAvD_BwE">Purchase Link</a>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-2-3">
						<h2 class="section-heading">Gyvazla USB-microphone</h2>
						<p>The microphone could be set up easily via a USB interface. For example, to use the microphone with the <a href="#item-7-7">script</a>
							<br>One needs to get the device index<br><br>
							>>> import pyaudio<br>
							>>> p = pyaudio.PyAudio()<br>
							>>> for ii in range(p.get_device_count()):<br>
							>>>     print(p.get_device_info_by_index(ii).get('name'))<br><br>
							<a href="https://makersportal.com/blog/2018/8/23/recording-audio-on-the-raspberry-pi-with-python-and-a-usb-microphone">Reference</a></p>
					</section>
					<figure class="figure docs-figure py-1">
						<a href="assets/images/microphone.jpg" data-title="Gyvazla USB-microphone"
							data-toggle="lightbox"><img class="figure-img img-fluid shadow rounded"
								src="assets/images/microphone.jpg#" alt="" style="width: 200px;"></a>
						<figcaption class="figure-caption mt-3"></i>Gyvazla USB-microphone</figcaption>
					</figure>
					<br><a href="https://www.amazon.de/dp/B08XQ4XKWF/ref=sspa_dk_detail_0?psc=1&pd_rd_i=B08XQ4XKWF&pd_rd_w=3Ee8H&pf_rd_p=80f139ed-36cd-4376-aa65-33f3cf0faca3&pd_rd_wg=STlDW&pf_rd_r=JZGFQ2Y37S9V6J6KHVT8&pd_rd_r=0f822a78-dc7f-491b-9ce1-f57b4017ee50&smid=A2ANY0W86ZG2O8&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUE3MFM2SEJPN1dMRjUmZW5jcnlwdGVkSWQ9QTAzNDAxNjIyWU83MThQMTNOWU1GJmVuY3J5cHRlZEFkSWQ9QTAxODc4NTczSFhBNzM4QkFIOFM5JndpZGdldE5hbWU9c3BfZGV0YWlsJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==">Purchase Link</a>
					<!--//section-->
				</article>
				<!--//docs-article-->


				<article class="docs-article" id="section-3">
					<header class="docs-header">
						<h1 class="docs-heading">Technology Stack</h1>
						<section class="docs-intro">
							<p>The list of some major libraries is given below:
								<table class="table">
    <thead>
      <tr>
        <th>Libaray</th>
        <th>Version</th>
      </tr>
    </thead>
    <tbody>
    <tr>
    <td>Python</td>
    <td>3.7.3</td>
    </tr>
      <tr>
        <td>Flask</td>
        <td>1.0.2</td>
      
      </tr>
      <tr>
        <td>numpy</td>
        <td>1.16.2</td>
      </tr>
      <tr>
        <td>PyAudio</td>
        <td>0.2.11</td>
      </tr>
            <tr>
        <td>pyfcm</td>
        <td>1.5.1</td>
      </tr>
            <tr>
        <td>tensorflow</td>
        <td>2.4.0</td>
      </tr>
    </tbody>
  </table>
							<br>
								<br><br>This section includes detailed information about the different technologies used in the project starting form python to tensorflow, which was used for developing the deep learning model to classify the various sounds that a dog makes.</p>
						</section>
						<!--//docs-intro-->
					</header>
					<section class="docs-section" id="item-3-1">
						<h2 class="section-heading">Python</h2>
						<p>Python is a programming language that lets you work more quickly and integrate your systems more effectively. <a target="_blank" href="https://www.python.org/">[1]</a> We decided to use Python for our project since there were various technologies to be used in the project. Since, it is not feasible to use different programming languages for one project that need to be interpolated with different techniques. </p>
						<figure class="figure docs-figure py-3">
							<a target="_blank" href="https://www.python.org/"
								 data-title="Python" data-toggle="lightbox"><img
									class="figure-img img-fluid shadow rounded"
									src="assets/images/Python.svg.png" alt=""
									style="width: 100px;"></a>
							<figcaption class="figure-caption mt-3"></i></figcaption>
						</figure>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-3-2">
						<h2 class="section-heading">Tensorflow</h2>
						<p>Tensorflow is one of the famous deep learning libraries available in the market to perform tasks like linear regression, image classification, etc.</p>
						<figure class="figure docs-figure py-3">
							<a target="_blank" href="https://www.tensorflow.org/"><img
									class="figure-img img-fluid shadow rounded"
									src="assets/images/Tensorflow_logo.svg.png" alt=""
									style="width: 100px;"></a>
							<figcaption class="figure-caption mt-3"></i></figcaption>
						</figure>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-3-3">
						<h2 class="section-heading">Firebase Cloud Messaging</h2>
						<p>Firebase Cloud Messaging is a feature available from Firebase which enables us to send push notifications across various devices once a device has been registered through a service worker. Some examples of push notifications are the ones that you get from various apps like facebook(getting a new friend request), amazon(order on the way, order dispatched), etc.</p>
						<figure class="figure docs-figure py-3">
							<a target="_blank" href="https://firebase.google.com/docs/cloud-messaging"
								 ><img
									class="figure-img img-fluid shadow rounded"
									src="assets/images/fcm.jpeg" alt=""
									style="width: 300px;"></a>
							<figcaption class="figure-caption mt-3"></i></figcaption>
						</figure><br>
						<figure class="figure docs-figure py-1">
							<a href="assets/images/push.png" data-title="Gyvazla USB-microphone"
								 data-toggle="lightbox"><img class="figure-img img-fluid shadow rounded"
																						 src="assets/images/push.png#" alt="" style="width: 200px;"></a>
							<figcaption class="figure-caption mt-3"></i>Push Notification Example</figcaption>
						</figure>
					</section>
					<!--//section-->
				</article>
				<!--//docs-article-->

				<article class="docs-article" id="section-4">
					<header class="docs-header">
						<h1 class="docs-heading">Demo 1</h1>
						<section class="docs-intro">
							<p>Here is an overview of Demo 1.</p>
						</section>
						<!--//docs-intro-->
					</header>
					<section class="docs-section" id="item-4-1">
						<h2 class="section-heading">Setting up Raspberry</h2>
						<p>Raspberry Pi 4 with UI was setup for the project. Vnc viewer was used to access the UI.
							<figure class="figure docs-figure py-3">
								<a target="_blank" href="assets/images/vnc.png"
									 data-title="Raspberry Pi 4 Model B" data-toggle="lightbox"><img
										class="figure-img img-fluid shadow rounded"
										src="assets/images/vnc.png" alt=""
										style="width: 400px;"></a>
								<figcaption class="figure-caption mt-3"></i>VNC Viewer</figcaption>
							</figure>
						</p>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-4-2">
						<h2 class="section-heading">Microphone Setup</h2>
						<p>For our project we required a microphone to record the sound from a dog. The first attempt was made with a sound module.
						Upon inspecting it was found out that there isn't a possibility of saving the audio recorded from the microphone and it can only play audio on the go. Since, our use case required us to save the audio file for sending it to the api to analyse the sound, we decided to look for some other device.
						</p>
						<figure class="figure docs-figure py-3">
							<a target="_blank" href="https://www.reichelt.de/de/de/entwicklerboards-soundmodul-rec-play-isd1820-debo-soundmodule-p202833.html?PROVID=2788&gclid=Cj0KCQjw38-DBhDpARIsADJ3kjn3cWooO6GfgpjUUjv-k1GHM9vKwKvv_Hs7yTnkPsliDyREm5ZtnIUaAqvdEALw_wcB&&r=1"><img
									class="figure-img img-fluid shadow rounded"
									src="assets/images/somd.jpeg" alt=""
									style="width: 300px;"></a>
							<figcaption class="figure-caption mt-3"></i></figcaption>
						</figure>
						<p>Next, we tried to setup the Bluetooth Device mentioned <a href="#item-2-2">above.</a> The function was successful but considering our project it wouldn't have been optimal to choose the device because we required to record sound and also play music simultaneously. The issue was that there needs to made a protocol switch on the device for playing and recording;  <a target="_blank" href="https://en.wikipedia.org/wiki/List_of_Bluetooth_profiles#Headset_Profile_(HSP)">HSP</a> and <a target="_blank" href="https://en.wikipedia.org/wiki/List_of_Bluetooth_profiles#Advanced_Audio_Distribution_Profile_(A2DP)">A2DP</a> protocols. </p>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-4-3">
						<h2 class="section-heading">Audio Output</h2>
						<p>We were able to connect our bluetooth sound speaker to our Raspberry Pi. Following is a video showing the audio output from the Raspberry to the bluetooth speaker.</p>
						<iframe src="https://drive.google.com/file/d/1CwaA-6nu6auzz_elji_-Rdzjv02vYGK_/preview" width="640" height="480" allow="autoplay"></iframe>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-4-4">
						<h2 class="section-heading">Data collection for Tensorflow</h2>
						<p>For generating a model to classify audio we required to collect different sounds which correspond to the sounds that dogs make. We used the dataset by <a target="_blank" href="https://research.google.com/audioset/ontology/dog_1.html">research.google.com</a><br>On every category page, we grabbed the json response which contained the audio files with timestamps.<br>For example, <a target="_blank" href="https://storage.googleapis.com/audioset_website_data/youtube_corpus/v1/balanced_train/bark/1.js">https://storage.googleapis.com/audioset_website_data/youtube_corpus/v1/balanced_train/bark/1.js</a> gives a json response. We wrote a small javascript to take that response and give a list directly copeable into an <a target="_blank" href="https://docs.google.com/spreadsheets/d/1aNIikUhNj3v6oarXHSIENYtTNHg-NxNrSj4HN4miZZc/edit#gid=0">excel sheet.</a></p>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-4-5">
						<h2 class="section-heading">Test Api Structure</h2>
						<p>To have an idea of how the Dog sound classifier would analyse and respond to an audio file, a test api was created using <a target="_blank" href="https://testapi.io/">https://testapi.io/</a>
							<br>Responses:<br><br>When the sound classifies as one of separation anxity: <a target="_blank" href="https://testapi.io/api/pulkit604/dog_sound_classify">https://testapi.io/api/pulkit604/dog_sound_classify
							</a><br>Output: { result: true }<br>When the sound doesn't classify as one of separation anxity:  <a target="_blank" href="https://testapi.io/api/pulkit604/dog_sound_classify_1">https://testapi.io/api/pulkit604/dog_sound_classify_1
							</a><br>Output: { result: false }
						</p>
					</section>
					<!--//section-->
				</article>
				<!--//docs-article-->


				<article class="docs-article" id="section-5">
					<header class="docs-header">
						<h1 class="docs-heading">Demo 2</h1>
						<section class="docs-intro">
							<p>Here is an overview of Demo 2.</p>
						</section>
						<!--//docs-intro-->
					</header>
					<section class="docs-section" id="item-5-1">
						<h2 class="section-heading">Problems in Demo 1</h2>
						<p>There were a bunch of issues faced by the time of demo 1. They are listed below:<br>
						<ul>
						<li>Saving the audio to the Raspberry Pi was not possible because it doesn't allow to save input and only can play directly into the speaker.</li>
						<li>Audio recording to Raspberry wasn't possible</li>
						<li>Using bluetooth device for recording and playing music wasn't successful due to A2DP and HSP protocol switching issue (Solved by using a separate usb mic for recording)</li>
					</ul></p>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-5-2">
						<h2 class="section-heading">Mic + Speaker Function</h2>
						<p>The recording function and calm music playback function was separated out by including a <a href="#item-2-3">USB Microphone</a>. The recording then works on the usb microphone and music playback on the bluetooth speaker. The video below shows the functioning(At first, the laptop on the left plays a dog barking sound and then the bluetooth speaker plays it backs through the recording made by the Raspberry Pi):</p>
						<iframe src="https://drive.google.com/file/d/1ymgNo2kSGAjeqSbNLNABikile-GiWulC/preview" width="640" height="480" allow="autoplay"></iframe>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-5-3">
						<h2 class="section-heading">Full Use Case</h2>
						<p>The use case demontrated in demo 2 is summarized below:
						<ol>
						<li>Dog sounds are played from the laptop on the left hand side</li>
						<li>Sound is recorded through the usb microphone and saved onto the Raspberry Pi (shown in the Python Code)</li>
						<li>After the successful recording an API call to the sample api was made(later it would me made to a flask server running tensorflow on it)</li>
						<li>After the APi call was made, a music file containing calm music is played on the bluetooth speaker through the Raspberry Pi</li>
						<li>At the end, a push notification is sent onto the mobile device using FCM</li>
					</ol></p>
						<figure class="figure docs-figure py-3">
							<a target="_blank" href="assets/images/fcase.png"
								 data-title="Raspberry Pi 4 Model B" data-toggle="lightbox"><img
									class="figure-img img-fluid shadow rounded"
									src="assets/images/fcase.png" alt=""
									style="width: 400px;"></a>
							<figcaption class="figure-caption mt-3"></i>Full use case view</figcaption>
						</figure><br>
						<iframe src="https://drive.google.com/file/d/1qME2t_iuvqucpz5hBn5wfrp-zsK6bg29/preview" width="640" height="480" allow="autoplay"></iframe>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-5-4">
						<h2 class="section-heading">Python script to showcase features</h2>
						<p>A python script was written to showcase all the features of the project. Below is the script: <br>
							The process can be understood as:<br>
						<ol>
							<li>Libraries like pygame, pyaudio, requests, pyfcm, etc. are loaded which are helpful for the different functions</li>
							<li>The different functions are defined namely record(record audio using the usb microphone), call_api(make request to the flask api), play(stream calm music to the bluetooth speaker), send_push(send a push notification to mobile device)</li>
							<li>record(): This method makes a 16 bit, single channel recording, using device index 2(usb microphone) using pyaudio library and saves it into test1.wav</li>
							<li>call_api(): https://testapi.io/api/pulkit604/dog_sound_classify is the address that is called using the requests api</li>
							<li>play(): pygame mixer library is initialised and loaded with calm_scrott.mp3 and is then played until it finishes</li>
							<li>send_push(): Push service is declared with api key from the fcm project, registration token of the device is set and then the push notification is sent with the details set</li>
							<li>The script runs once and executes all the 4 functions in order</li>
						</ol>
							<script src="http://gist-it.appspot.com/https://github.com/pulkit604/dogMonitorDocs/blob/master/python_scripts/all_in_one_demo2.py"></script>
						</p>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-5-5">
						<h2 class="section-heading">Website for Push Notifications</h2>
						<p>One of the features in our project was to notify the user when the music isn't able to calm down the sad dog. We had two options to choose from: <br> Sms and push notifications <br> We chose Push Notifications since in the modern world, people tend to be more reactive towards push notifications due to the ease. We built a basic Vue Website Project
							<a target="_blank" href="https://github.com/pulkit604/dog-pwa
">Github Project Link</a>
							<figure class="figure docs-figure py-3">
								<a target="_blank" href="https://eloquent-bell-de424d.netlify.app/"
									 data-title="Dog Push Website" data-toggle="lightbox"><img
										class="figure-img img-fluid shadow rounded"
										src="assets/images/pwa.png" alt=""
										style="width: 500px;"></a>
								<figcaption class="figure-caption mt-3"></i></figcaption>
							</figure>
							<br> and setup <a target="_blank" href="#item-3-3">Firebase Cloud Messaging</a> to send push notifications to devices easily from a python script. </p>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-5-6">
						<h2 class="section-heading">Data collection update</h2>
						<p>The files downloaded in demo 1 were audios from complete youtube videos(3-4 minutes, for example). We required to trim the audio files and also generate an excel file with the remaining files since some of the youtube videos were already deleted or private. Below is the script:</p>
						<script src="http://gist-it.appspot.com/https://github.com/pulkit604/dogMonitorDocs/blob/master/python_scripts/trim_demo2.py"></script>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-5-7">
						<h2 class="section-heading">Bitrate change</h2>
						<p>The model required audio files to be transformed into 16 bitrate sound files. Hence, the script was written which takes up all the files and converts them to a bitrate of 16.
							<br><br><a href="#item-7-2">Link</a></p>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-5-8">
						<h2 class="section-heading">Tensorflow model update</h2>
						<p>We tried a couple of existing models for our project but weren't successful with that. Here are two projects, that we tried:<br>
							<a target="_blank" href="https://github.com/GianlucaPaolocci/Sound-classification-on-Raspberry-Pi-with-Tensorflow">Urban Sound Classfier Project Link</a>
							<br><a target="_blank" href="https://teachablemachine.withgoogle.com/">Teachable Machine Project Link</a>
						<br><br>Finally, we were able to find a model by tensorflow which was properly documented and was easy to possible to use our downloaded audio wav files with some modifications. <a target="_blank" href="https://www.tensorflow.org/tutorials/audio/simple_audio"><br>Project Link</a><br><br>
							The model works by transforming the audio files into waveforms and then into spectrogram images. And when the model is trained from these images, it works on the concept of image classification by determining the similarity between the images of different categories/labels.
							<br>Below are some results of waveforms and spectrograms from the different categories that we had from the dog audio dataset.
							<br><figure class="figure docs-figure py-3">
								<a target="_blank" href="assets/images/wave.png"
									 data-title="Waveforms" data-toggle="lightbox"><img
										class="figure-img img-fluid shadow rounded"
										src="assets/images/wave.png" alt=""
										style="width: 400px;"></a>
								<figcaption class="figure-caption mt-3"></i>Waveforms</figcaption>
							</figure>
							<br><figure class="figure docs-figure py-3">
								<a target="_blank" href="assets/images/spec.png"
									 data-title="Spectrograms" data-toggle="lightbox"><img
										class="figure-img img-fluid shadow rounded"
										src="assets/images/spec.png" alt=""
										style="width: 400px;"></a>
								<figcaption class="figure-caption mt-3"></i>Spectrograms</figcaption>
							</figure>
						</p>
					</section>
					<!--//section-->
				</article>
				<!--//docs-article-->


				<article class="docs-article" id="section-6">
					<header class="docs-header">
						<h1 class="docs-heading">Final Version</h1>
						<section class="docs-intro">
							<p>After a lot of experimentation and playing around with audio files of dogs barking, we were able to generate a model and deploy it on the Raspberry Pi itself and serve through a flask server. Below are the detailed infos.</p>
						</section>
						<!--//docs-intro-->
					</header>
					<section class="docs-section" id="item-6-1">
						<h2 class="section-heading">Dataset Issues and final workaround</h2>
						<p>There came to be a lot of issues with the audio files like: <br>
						<ul>
						<li>The majority of the sound files had terrible noises(people talking, shouting, musical instruments) in the background</li>
						<li>The datasets had a poor categorization specially for the categories Yip and Whimper, which are the most imporant for us, since these two are the ones that correspond to the sound made by a sad dog relating to separation anxiety.</li>
						<figure class="figure docs-figure py-3">
							<a target="_blank" href="assets/images/poor_acc.png"
								 data-title="Raspberry Pi 4 Model B" data-toggle="lightbox"><img
									class="figure-img img-fluid shadow rounded"
									src="assets/images/poor_acc.png" alt=""
									style="width: 400px;"></a>
							<figcaption class="figure-caption mt-3"></i>Poor Quality dataset</figcaption>
						</figure>
					</ul>
						<br>So, due to that fact, we only took the evaluation (for ex. <a target="_blank" href="https://research.google.com/audioset//eval/bark.html">Bark Evaluation videos</a> )and evaluation videos (for ex., <a target="_blank" href="https://research.google.com/audioset//balanced_train/bark.html">Bark Balanced Videos</a> ) for each type(avg. 100 videos for each category) in order make a model that could at least have accuracy.
						Here, is the collection copied into an <a target="_blank" href="https://docs.google.com/spreadsheets/d/1jZxZN6x3-CqUoS3p9jMzvOOMGyxAsaIyQkZ3bTEOCaI/edit?usp=sharing">excel file.</a>
						<br>After the collection was made, there still existed a problem of unwanted noise in the better videos too. In order to tackle this issue, we manually filtered out the audio files by listening to the audio files which weren't good for the training and deleted them one by one. We didn't go for some tool to filter it out because it wouldn't be easy to filter out dog sounds from noise as the tool could identify both as noise.
						<br>At the end, we were left with these <a target="_blank" href="https://docs.google.com/spreadsheets/d/1jZxZN6x3-CqUoS3p9jMzvOOMGyxAsaIyQkZ3bTEOCaI/edit?usp=sharing">files</a>.
						</p>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-6-2">
						<h2 class="section-heading">Final Model build and results</h2>
						<p>After we were done with filtering out the audio files and get the best set, we started with training the convolutional neural network(CNN). There were some issues when starting with training. The issues and their solutions are noted below:</p>
						<ol>
							<li>Firstly, the audio files were complete audios from youtube videos(2-4 minutes), it was necessary to be crop them to the timestamps provided by the audioset. This <a href="#item-7-4">script</a> was used for that</li>
							<li>After that, there was an issue with the audio files that it didn't support 32 bit audio files. For, that we wrote a script to convert the audio files into 16-bit wav audio files
								<br> The link to the script is <a href="#item-8-3">here</a>:</li>
							<li>After, the bitrate conversion there came a new issue that the audio files had to be single channel. The script for that is attached <a href="#item-7-4">here</a>: </li>
							<li>One more problem occured that while training the generated audio files were inconsistent in the dimensions of the spectrograms generated. These were ignored while training by writing simple ignore statement in for loops</li>
							<li>At the end, since the number of audio files were limited, we created only two labels (yes - corresponding to a sad dog; no - not that case of a sad dog) instead of 6 categories because that anyways would be very less accurate due to the less number of files per category</li>
							<li>Upon the successful training of the model, we were able to generate a model with not very good accuracy to distinguish between the two categorical sounds but upon a little experimentation we found out that the difference between the probability of `yes` and `no` was less than 0.21 in most cases. We later on used that observation as a deciding factor for the api</li>
						</ol>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-6-3">
						<h2 class="section-heading">Model Deploy and Flask Server</h2>
						<p>After the model creation, we were to decide how we could use it in an Api, which could be similar to the <a href="#item-4-5">test api</a> and could be called using python requests library. After the model was built, the size came up to be around 20 Mb.<br>
						It was small enough to be used within the Raspberry Pi. So, next we tried with different versions of tensorflow that would be supported on the raspberry. After a lot of experimentation with different versions, we found version 2.4.0 to be compatible.<br>
						<br>We created a flask server, which loads the generated model using keras <a target="_blank" href="https://www.tensorflow.org/tutorials/keras/save_and_load">load_model</a> function and sent response similar to the test api created earlier after reading the file's contents. The script can be found here: <a href="#item-7-6">Script Link</a> </p>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-6-4">
						<h2 class="section-heading">Continous Listening and Break Script</h2>
						<p>As part of the use case, we had planned to make a script that runs continuously, listens to the sounds and analyses and when at second attempt doesn't work sends out a push notification to the device and exits itself.<br>
							<a href="#item-7-7">This</a> is the script. </p>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-6-5">
						<h2 class="section-heading">Final Demo</h2>
						<p>Below is a final working of the functioning of the project. The process is as follows:
						<ol>
						<li>First of all, the flask server which used tensorflow loaded model is started</li>
						<li>Next, we run the continous running script which starts by recording sound which comes from the laptop connected on the right,</li>
						<li>Then, the api from flask server is called and since, the result is True,</li>
						<li>Calm music is played onto the bluetooth speaker</li>
						<li>Now, the script repeats the process of recording and calling api, </li>
						<li>Then the result is again True so calm music is played as well a push notification is sent on the mobile device</li>
						<li>After that the script stops running</li>
					</ol><br>
						<iframe src="https://drive.google.com/file/d/1jLOuuMB_7veu7Kz_LhQhAHGIz5FpAUAH/preview" width="640" height="480" allow="autoplay"></iframe><br></p>
					</section>
					<!--//section-->
				</article>
				<!--//docs-article-->

				<article class="docs-article" id="section-7">
					<header class="docs-header">
						<h1 class="docs-heading">Python Scripts</h1>
						<section class="docs-intro">
							<p>A bunch of python scripts were written and modified  for various use cases ranging from downloading youtube videos to stripping audio files to a certain timestamp or for the deep learning model. All of the scrips are described and attached below: </p>
						</section>
						<!--//docs-intro-->
					</header>
					<section class="docs-section" id="item-7-1">
						<h2 class="section-heading">Audioset Downloader</h2>
						<p>The below script reads an excel file with youtube video links and fetches audio files for that and saves to folders like Bark, Yip, etc. </p>
						The process can be understood as:<br>
						<ol>
							<li>An excel file with rows in the form->  category_id  audio_id start_timestamp end_timestamp is read using pandas.</li>
							<li>The pafy library is then used to fetch the youtube video and get the audio track from it and saved in the format "./" + category +  '/' + f'{audio_id}_start_{start_time}_end_{end_time}{bestaudio.extension}</li>
							<li>ffmpy is used to convert the audio, if the file is not in wav format</li>
						</ol>
						<script src="http://gist-it.appspot.com/https://github.com/pulkit604/dogMonitorDocs/blob/master/python_scripts/audioset-downloader.py"></script>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-7-2">
						<h2 class="section-heading">Bitrate Converter</h2>
						<p>The below script takes audio files and converts them a bitrate of 16.</p>
						The process can be understood as:<br>
						<ol>
							<li>Using Os library the files existing in the six folders(Bark, Yip, etc.) are listed</li>
							<li>After that the files are read using soundfile library simultaneously </li>
							<li>The files are then converted to PCM_16 which is a 16 bitrate format supported by the library and saved</li>
						</ol>
						<script src="http://gist-it.appspot.com/https://github.com/pulkit604/dogMonitorDocs/blob/master/python_scripts/bitrate_convert.py"></script>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-7-3">
						<h2 class="section-heading">Crop From Timestamp</h2>
						<p>The below script takes audio files and crops them to the start and endtimestamp mentioned in the file name like _fTONy_pqik_start_[start_timestamp]_end_[end_timestamp].wav</p>
						The process can be understood as:<br>
						<ol>
							<li>Using Os library the files existing in the six folders(Bark, Yip, etc.) are listed</li>
							<li>The names are split into link, start time, end time</li>
							<li>The files are loaded using AudioSegment library</li>
							<li>The cropped parts are taken like -> song[int(st)*1000:int(et)*1000], where st is the start time and et is the end time; 1000 is multiplied because milliseconds is the expected format</li>
							<li>At the end, the cropped parts saved by export function</li>
						</ol>
						<script src="http://gist-it.appspot.com/https://github.com/pulkit604/dogMonitorDocs/blob/master/python_scripts/crop_from_timestamp.py"></script>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-7-4">
						<h2 class="section-heading">Set Mono Sound Channel</h2>
						<p>To work with the model it was required that the audio files be single channel audio files. The below script does that function.</p>
						The process can be understood as:<br>
						<ol>
							<li>Using Os library the files existing in the six folders(Bark, Yip, etc.) are listed</li>
							<li>The files are loaded using AudioSegment library</li>
							<li>The channel is changed by `set_channels(1)`</li>
							<li>At the end, the cropped parts saved by export function</li>
						</ol>
						<script src="http://gist-it.appspot.com/https://github.com/pulkit604/dogMonitorDocs/blob/master/python_scripts/set_sound_channel_mono.py"></script>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-7-5">
						<h2 class="section-heading">Strip to 9 Seconds</h2>
						<p>There was a problem when generating the audio files to spectrogram images, that the images need to be of the same dimensions when training the model. Thus, the below script makes it possible to strip the audio files to the same duration of 9 seconds. </p>
						The process can be understood as:<br>
						<ol>
							<li>Using Os library the files existing in the six folders(Bark, Yip, etc.) are listed</li>
							<li>The cropped parts are taken like -> 0:9000]</li>
							<li>At the end, the cropped parts saved by export function</li>
						</ol>
						<script src="http://gist-it.appspot.com/https://github.com/pulkit604/dogMonitorDocs/blob/master/python_scripts/strip_9_sec.py"></script>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-7-6">
						<h2 class="section-heading">Flask server</h2>
						<p>The below script is the flask server application used to analyse the recorded file and respond with true or false according to the logic if the difference between the probability of `yes` and `no` is less than 0.21</p>
						The process can be understood as:<br>
						<ol>
							<li>Flask and tensorflow are the major libraries that are imported</li>
							<li>A flask app is declared</li>
							<li>Then a route is declared which would be accessible by a browser or using python's request library by http://127.0.0.0:5001/check</li>
							<li>The CNN model is loaded using keras' load_model function</li>
							<li>The recorded file test1.wav is tested with the model</li>
							<li>From the prediction made by the model, the difference of probabilities of yes and no is calculated and true is return as a response if the difference is less than 0.21 else false otherwise</li>
						</ol>
						<script src="http://gist-it.appspot.com/https://github.com/pulkit604/dogMonitorDocs/blob/master/python_scripts/flask_app.py"></script>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-7-7">
						<h2 class="section-heading">Final script for Raspberry Pi</h2>
						<p>The final script which records, checks the recording, plays music and sends push notifications is attached below.</p>
						The process can be understood as:<br>
						<ol>
							<li>Libraries like pygame, pyaudio, requests, pyfcm, etc. are loaded which are helpful for the different functions</li>
							<li>The different functions are defined namely record(record audio using the usb microphone), call_api(make request to the flask api), play(stream calm music to the bluetooth speaker), send_push(send a push notification to mobile device)</li>
							<li>record(): This method makes a 16 bit, single channel recording, using device index 2(usb microphone) using pyaudio library and saves it into test1.wav</li>
							<li>call_api(): http://127.0.0.1:5000/check is the address that is called using the requests api</li>
							<li>play(): pygame mixer library is initialised and loaded with calm_scrott.mp3 and is then played until it finishes</li>
							<li>send_push(): Push service is declared with api key from the fcm project, registration token of the device is set and then the push notification is sent with the details set</li>
							<li>The script runs until exit() function is called or is stopped manually using ctrl+c, for example.</li>
							<li>Starting step is recording of music file, the file is saved to test1.wav and is of 9 seconds</li>
							<li>The flask api is called and if the result has 'True', the count of times(the no. of trues) is increased; calm music is played</li>
							<li>After that process repeats itself until a 'True' response comes again</li>
							<li>When a second time 'True' comes, calm music is played and a push notification is sent to the mobile device and also the script is stopped from executing further by using exit() function</li>
							</ol>
						<script src="http://gist-it.appspot.com/https://github.com/pulkit604/dogMonitorDocs/blob/master/python_scripts/all_in_one.py"></script>
					</section>
					<!--//section-->

					<section class="docs-section" id="item-7-8">
						<h2 class="section-heading">Audio classifier Model</h2>
						<p>The below jupyter lab contains the modified code to load audio files and generate a classifier model to distinguish b/w dog related separation anxiety sound </p>
						<a target="_blank" href="https://github.com/pulkit604/dogMonitorDocs/blob/master/python_scripts/simpe_audio.ipynb">Github Link</a><iframe src="assets/simple_audio.html" width="100%" height="50%" ></iframe></a>
					</section>
					<!--//section-->
				</article>

				<article class="docs-article" id="section-8">
					<header class="docs-header">
						<h1 class="docs-heading">Observations and Future Scope</h1>
						<section class="docs-intro">
							<p>Creating models can generally take around 1-2 years to get it to be very efficient and accurate. Below are some observations made from the experimentation and how the project can be extended and modified for future use. </p>
							Some issues that we faced during the working time of project:
							<ul>
								<li>The audio set had a weak labelling of the different sound categories.</li>
								<li>The majority of the sound files had terrible noises(people talking, shouting, musical instruments) in the background.</li>
								<li>The audio files didn't have similar pattern as in, some had 3-4 seconds silence and then a sound or the other way round.</li>
								<li>There had to be done a manual filtering of the audio files by listening to them manually which takes a lot of time.</li>
								<li>For the model to accept the audios there had to be done a lot of operations like bitrate change, stereo->mono channel change which makes the final files a lot different from the original sound.</li>
							</ul>
							<br>There can be a lot of amendments possible with the project structure created using a better audio dataset.
							<ul>
								<li>If there can be added the factor of pitch from the audio files, it would be easier to have better accuracy since the sad sounds are generally told be of lower pitch than otherwise.</li>
								<li>If there could be a dataset which consists of sounds without noise, it would be easier to get better results.</li>
								<li>Since, there exists a wide variety of breeds of dogs, the project could be expanded by grouping some breeds of dogs that make similar sounds.</li>
								<li>Since, there are similar situations with little kids and babies. If there exists or would exist a dataset of small children crying, the same project could be used just by altering the audio files and could be directly utilized.</li>
							</ul>
						</section>
						<!--//docs-intro-->
					</header>
				</article>

				<article class="docs-article" id="section-9">
					<header class="docs-header">
						<h1 class="docs-heading">References</h1>
						<section class="docs-intro">
							<p>
								Here is a list of links that were referred during the creation of the document and project.
							<ul>
								<li><a href="https://www.python.org/">Python Official Site</a></li>
								<li><a href="https://www.tensorflow.org/">Tensorflow Official Site</a></li>
								<li><a href="https://wiki.debian.org/PulseAudio">Pulse audio wiki</a></li>
								<li><a href="https://firebase.google.com/docs/cloud-messaging">Firebase Cloud Messaging Introduction</a></li>
								<li><a href="https://en.wikipedia.org/wiki/List_of_Bluetooth_profiles#Headset_Profile_(HSP)">Bluetooth Profiles</a></li>
								<li><a href="https://research.google.com/audioset/ontology/dog_1.html">Google Research Dog Sound Dataset</a></li>
								<li><a href="https://makersportal.com/blog/2018/8/23/recording-audio-on-the-raspberry-pi-with-python-and-a-usb-microphone">Recording Audio on the Raspberry Pi with Python and a USB Microphone</a></li>
								<li><a href="https://testapi.io/">Testapi - Mock Api Tool</a></li>
								<li><a href="https://github.com/GianlucaPaolocci/Sound-classification-on-Raspberry-Pi-with-Tensorflow">Urban Sound Classifier Project</a></li>
								<li><a href="https://teachablemachine.withgoogle.com/">Teachable Machine Project by Google</a></li>
								<li><a href="https://www.tensorflow.org/tutorials/audio/simple_audio">Audio Classification Project by Tensorflow</a></li>
								<li><a href="https://www.tensorflow.org/tutorials/keras/save_and_load">Keras Save and Load Functionality</a></li>
								<li><a href="https://github.com/lhelontra/tensorflow-on-arm/releases">Tensorflow Versions</a></li>
								<li><a href="https://github.com/sunzhongbai/audioset-downloader">Audioset Downloader Project</a></li>
								<li><a href="https://github.com/jiaaro/pydub">Audio Manipulation Library</a></li>
								<li><a href="https://pypi.org/project/pyfcm/">Python Package to send Push Notifications</a></li>
								<li><a href="https://www.youtube.com/results?search_query=calm+music+dog">Youtube Search Query for Calming Music for Dogs</a></li>
							</ul>
						</section>
						<!--//docs-intro-->
					</header>
				</article>

				<footer class="footer">
					<div class="container text-center py-5">
						<!--/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */-->
						<small style="font-size: 150%">Edited by Pulkit Gupta and Serkan Akyildiz</small><br>
						<small class="copyright">Designed with <i class="fas fa-heart" style="color: #fb866a;"></i> by
							<a class="theme-link" href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying
								Riley</a> for developers</small>
					</div>
				</footer>
			</div>
		</div>
	</div>
	<!--//docs-wrapper-->



	<!-- Javascript -->
	<script src="assets/plugins/jquery-3.4.1.min.js"></script>
	<script src="assets/plugins/popper.min.js"></script>
	<script src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>


	<!-- Page Specific JS -->
	<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.8/highlight.min.js"></script>
	<script src="assets/js/highlight-custom.js"></script>
	<script src="assets/plugins/jquery.scrollTo.min.js"></script>
	<script src="assets/plugins/lightbox/dist/ekko-lightbox.min.js"></script>
	<script src="assets/js/docs.js"></script>

</body>

</html>
